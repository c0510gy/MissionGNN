# MissionGNN â€“ Knowledgeâ€‘Graphâ€‘Driven Temporal Reasoning for Video Anomaly Detection

[![WACVÂ 2025](https://img.shields.io/badge/WACV-2025-7957D5.svg)](https://arxiv.org/abs/2406.18815)Â Â [![PythonÂ 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)]()

> [**MissionGNN: Hierarchical Knowledgeâ€‘Graph Reasoning Meets Shortâ€‘Term Temporal Context for Surveillance Video Anomaly Detection**](https://arxiv.org/abs/2406.18815)
> *In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACVÂ 2025)*
> **HyunwooÂ Oh**, SanggeonÂ Yun, RyozoÂ Masukawa, NathanielÂ D.Â Bastian, MohsenÂ Imani

![MissionGNN overview1](figures/figure1.png)

<p align="center"><em>Figure&nbsp;1. The framework for mission-specific knowledge graph generation.</em></p>

![MissionGNN overview2](figures/figure2.png)

<p align="center"><em>Figure&nbsp;2. The overall framework for our proposed model utilizing the novel concept of hierarchical graph neural network.</em></p>

MissionGNN attaches a **sensor node** (video frame embedding) to a **classâ€‘specific knowledge graph (KG)** and propagates information through custom GCN layers.  Encodings from 13 KGs (one per UCFâ€‘Crime event) are concatenated and passed through a lightweight transformer to yield an **(NormalÂ + n events) classifier**.  A **decaying threshold** progressively suppresses lowâ€‘confidence positives during training, mimicking curriculum learning.

---

## ğŸ“ Repository Structure

```
MissionGNN/
â”œâ”€â”€ config.py              # Hyperâ€‘parameters, paths
â”œâ”€â”€ datasets.py            # 30â€‘frame slidingâ€‘window dataset
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ kg_loader.py       # Load & embed perâ€‘class KGs
â”‚   â””â”€â”€ layers.py          # Custom GCNConvTarget
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gcn.py             # Batched KnowledgeGCN
â”‚   â”œâ”€â”€ temporal.py        # Transformer + prediction head
â”‚   â””â”€â”€ missiongnn.py      # Endâ€‘toâ€‘end model
â”œâ”€â”€ train.py               # Training loop w/ decaying Ï„
â”œâ”€â”€ evaluate.py            # Checkpoint evaluation
â”œâ”€â”€ requirements.txt       # Tested library versions
â””â”€â”€ README.md              # You are here ğŸš€
```

---

## ğŸš€ Quickstart

### 1. Clone and install

```bash
git clone https://github.com/c0510gy/MissionGNN.git
cd MissionGNN
pip install -r requirements.txt
```

### 2. Prepare data & subâ€‘graphs

Place UCFâ€‘Crime frame embeddings (`*.pt`) under
`./embeddings/` and
perâ€‘class KG files under `./subgraphs/`:

```
subgraphs/
 â”œâ”€â”€ subgraph_Abuse.txt       # edge list: v->u per line
 â”œâ”€â”€ keywords_Abuse.txt       # key concept words (one per line)
 â”œâ”€â”€ subgraph_Arrest.txt
 â””â”€â”€ ...
```

### 3. Train

```bash
python train.py
```

### 4. Evaluate best checkpoint

```bash
python evaluate.py
```

Outputs:

* Training / validation loss, mAUC, mAP
* Best model in `checkpoints/best.pt`

---

## ğŸ”§ Key Configs (`config.py`)

```python
threshold_start = 1.0     # Start of decaying pseudoâ€‘label threshold
threshold_decay = 0.9999  # Threshold decay
embed_dim       = 1024    # ImageBind huge text/video embedding size
gnn_hidden      = 8       # perâ€‘node hidden dim inside KnowledgeGCN
```

Modify any field and reâ€‘run `train.py`; everything else adapts automatically.

---

## ğŸ“ Citation

```bibtex
@inproceedings{yun2025missiongnn,
  title={Missiongnn: Hierarchical multimodal gnn-based weakly supervised video anomaly recognition with mission-specific knowledge graph generation},
  author={Yun, Sanggeon and Masukawa, Ryozo and Na, Minhyoung and Imani, Mohsen},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  pages={4736--4745},
  year={2025},
  organization={IEEE}
}
```
